{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "nb2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junawaneshivani/Pytorch/blob/master/nb2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO288xhc0KYs",
        "colab_type": "text"
      },
      "source": [
        "# AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
        "- The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
        "- If you set your tensor's attribute `.requires_grad` as `True`, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n",
        "- To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked.\n",
        "- To prevent tracking history (and using memory), you can also wrap the code block in `with torch.no_grad()`:. This can be particularly helpful when evaluating a model.\n",
        "- Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a Function that has created the Tensor (except for Tensors created by the user - their grad_fn is None).\n",
        "- If you want to compute the derivatives, you can call .backward() on a Tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsTbfRII0KYt",
        "colab_type": "code",
        "colab": {},
        "outputId": "26fdfcab-bbf7-4ddb-d983-698a17b51379"
      },
      "source": [
        "import torch\n",
        "x = torch.ones(2, 2, requires_grad=True)              # start tracking computation\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yfqmVdH0KYx",
        "colab_type": "code",
        "colab": {},
        "outputId": "a682ccf3-31b5-40f5-a781-16fa9b10b966"
      },
      "source": [
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2v1o9za0KY0",
        "colab_type": "code",
        "colab": {},
        "outputId": "9ae17ad8-7552-49dc-9363-9fe3704e0351"
      },
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "print(z, out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTOUNsug0KY2",
        "colab_type": "code",
        "colab": {},
        "outputId": "26e03ba5-1a1f-4eb7-896b-6ed309bef948"
      },
      "source": [
        "out.backward()   # dout/dx\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs77nYNK0KY4",
        "colab_type": "text"
      },
      "source": [
        "we have that $out = \\frac{1}{4}\\sum_i z_i$,\n",
        "$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
        "Therefore,\n",
        "$\\frac{\\partial out}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n",
        "$\\frac{\\partial out}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ChlFsi0KY5",
        "colab_type": "code",
        "colab": {},
        "outputId": "7226a7eb-3cdc-42f0-f191-299d69c9fb4e"
      },
      "source": [
        "print(y.grad)    # will throw an error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhubhu180KY7",
        "colab_type": "code",
        "colab": {},
        "outputId": "e280b33d-417d-4d52-a833-cc4867232884"
      },
      "source": [
        "# Ways to disable gradient computation\n",
        "\n",
        "print(x.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x+2).requires_grad)\n",
        "\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "\n",
        "x.requires_grad_(False)\n",
        "print(x.requires_grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "False\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHNlTcMo0KY9",
        "colab_type": "text"
      },
      "source": [
        "**Reference** : https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhFn41FN0KY-",
        "colab_type": "text"
      },
      "source": [
        "More example: https://www.youtube.com/watch?v=DbeIqrwb_dE&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=4&t=0s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoSW9Y8S0KY_",
        "colab_type": "code",
        "colab": {},
        "outputId": "24c9bb96-dc89-43a8-bb07-21ec425680ac"
      },
      "source": [
        "# grads can be created only for scalar outputs\n",
        "x = torch.randn(2, 5, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 2\n",
        "#vec = torch.randn(2, 5)            # needed for jacobian product\n",
        "z.backward(vec)                    # z is not a scalar\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  1.9968,   0.4964, -12.6622,   4.3206,   1.0706],\n",
            "        [ -1.0943,   6.7481,  27.1264,  -2.9408,   0.1543]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkM6uSEx0KZB",
        "colab_type": "code",
        "colab": {},
        "outputId": "fd2aecca-50c4-4ca3-9d9e-82609a8908bb"
      },
      "source": [
        "# grads need to be reset every iteration as they get summed up\n",
        "\n",
        "weights = torch.ones(3, requires_grad=True)\n",
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    output = (weights * 3).sum() # grad of x * 3 = 3\n",
        "    output.backward()\n",
        "    \n",
        "    print(weights.grad)\n",
        "    #weights.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3.])\n",
            "tensor([6., 6., 6.])\n",
            "tensor([9., 9., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGGHzH_j0KZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you are using optimizer even then you need to make the grads 0 using below function\n",
        "\n",
        "optimizer = torch.optim.SGD(weights, lr=0.1)\n",
        "optimizer.step()\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1uG4eIt0KZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}